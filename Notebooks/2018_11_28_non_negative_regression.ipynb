{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdia import *\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be devoted to trying out algorithms for non-negative regression on spectra. Most of the spectral processing will be a recapitulation of the analyze function from the SpectralAnalyzer class. I will keep all of that up front so that I can spend the bulk of the notebook on analysis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../Data/DIA_FULL/UWPROFL0362.mzML\"\n",
    "file = MzMLFile()\n",
    "exp = MSExperiment()\n",
    "file.load(filename, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = SpectralAnalyzer(lib_path=\"../Data/spectral_library/\",\n",
    "                        lib_name=\"filtered_silac_library_b&y\",\n",
    "                        ppm_tol=5., lam=1e8, is_silac=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_data(frame_ind):\n",
    "    frame = exp[frame_ind]\n",
    "    \n",
    "    # Extract spectra\n",
    "    mz, intensity = spec.extract_spectra(frame)\n",
    "        \n",
    "    # Filter libraries\n",
    "    retention_range = frame.getRT() + 60 * 5. * np.array([-1., 1.])\n",
    "    precursor = frame.getPrecursors()[0]\n",
    "    mass_range = precursor.getMZ() + precursor.getIsolationWindowLowerOffset() * np.array([-1. , 1.])\n",
    "    filtered_lib = spec.lib.get_range(retention_range, mass_range)\n",
    "    filtered_decoys = spec.decoys.get_range(retention_range, mass_range)\n",
    "        \n",
    "    # Filter peaks\n",
    "    filtered_lib, lib_bins = spec.bin_peaks(mz, filtered_lib)\n",
    "    filtered_decoys, decoy_bins = spec.bin_peaks(mz, filtered_decoys)\n",
    "        \n",
    "    if filtered_lib.shape[0] == 0:\n",
    "        return None, None\n",
    "        \n",
    "    # Build columns\n",
    "    if spec.is_silac:\n",
    "        idx = filtered_lib.isotope.values\n",
    "        decoy_idx = filtered_decoys.isotope.values\n",
    "    else:\n",
    "        idx = filtered_lib.index.values\n",
    "        decoy_idx = filtered_decoys.index.values\n",
    "            \n",
    "    unique_idx, first_idx = np.unique(idx, return_index=True)\n",
    "    unique_decoy_idx, decoy_first_idx = np.unique(decoy_idx, return_index=True)\n",
    "        \n",
    "    mapping_dict = {old: new for new, old in enumerate(unique_idx)}\n",
    "    col_idx = np.array([mapping_dict[ind] for ind in idx], dtype=np.int64)\n",
    "        \n",
    "    decoy_mapping_dict = {old: new for new, old in enumerate(unique_decoy_idx)}\n",
    "    offset = col_idx.max() + 1\n",
    "    decoy_col_idx = np.array([decoy_mapping_dict[ind] + offset for ind in decoy_idx], dtype=np.int64)\n",
    "        \n",
    "    concat_cols = np.concatenate([col_idx, decoy_col_idx])\n",
    "        \n",
    "    # Build rows\n",
    "    unique_bins = np.unique(np.concatenate([lib_bins, decoy_bins]))\n",
    "    mapping_dict = {old: new for new, old in enumerate(unique_bins)}\n",
    "\n",
    "    row_idx = np.array([mapping_dict[ind] for ind in lib_bins])\n",
    "    decoy_row_idx = np.array([mapping_dict[ind] for ind in decoy_bins], dtype=np.int64)\n",
    "        \n",
    "    concat_rows = np.concatenate([row_idx, decoy_row_idx])\n",
    "        \n",
    "    # Build matrices\n",
    "    sparse_library = csc_matrix((np.concatenate([filtered_lib.intensity.values.flatten(),\n",
    "                                                 filtered_decoys.intensity.values.flatten()]), \n",
    "                                 (concat_rows, concat_cols)),\n",
    "                                shape=(concat_rows.max() + 1, concat_cols.max() + 1))\n",
    "    regression_library = sparse_library.todense()\n",
    "    target = intensity[unique_bins]\n",
    "    \n",
    "    # Create groups\n",
    "    if spec.is_silac:\n",
    "        group_idx = filtered_lib.index.values[first_idx]\n",
    "        decoy_group_idx = filtered_decoys.index.values[decoy_first_idx]\n",
    "        \n",
    "        unique_group = np.unique(group_idx)\n",
    "        group_map = {old: new for new, old in enumerate(unique_group)}\n",
    "        groups = np.array([group_map[pep] for pep in group_idx], dtype=np.int64)\n",
    "        \n",
    "        unique_decoy_group = np.unique(decoy_group_idx)\n",
    "        offset = len(unique_group)\n",
    "        decoy_group_map = {old: new + offset for new, old in enumerate(unique_decoy_group)}\n",
    "        groups = np.concatenate([groups, \n",
    "                                 np.array([decoy_group_map[pep] for pep in decoy_group_idx], \n",
    "                                          dtype=np.int64)])\n",
    "        return regression_library, target, groups\n",
    "    \n",
    "    else:\n",
    "        return regression_library, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 ms, sys: 2 ms, total: 23 ms\n",
      "Wall time: 22.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y, g = get_frame_data(30060)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Subgradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is given by the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^+} \\Big( \n",
    "\\| \\mathbf{y} - \\sum_{l=1}^L \\mathbf{X}_l \\beta_l \\|_2^2 +\n",
    "\\lambda_1 \\sum_{l=1}^L \\| \\beta_l \\|_2 +\n",
    "\\lambda_2 \\| \\beta \\|_1  \n",
    "\\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the subgradients being given by,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_{l,j}} Objective =\n",
    "- \\textbf{X}_{l,j}^T (\\textbf{r} - \\textbf{X}_{l,j} \\beta_{l,j}) +\n",
    "\\lambda_1 s_{l,j} +\n",
    "\\lambda_2 t_{l,j} =\n",
    "0\n",
    "$$\n",
    "\n",
    "$$ s_{l,j} =  \n",
    "\\begin{cases} \n",
    "      \\frac{\\beta_{l,j}}{\\|\\beta\\|_2} & \\beta \\neq 0 \\\\\n",
    "      \\textbf{s} \\in \\mathbb{R}^k \\quad s.t. \\quad \n",
    "      \\| \\textbf{s} \\|_2 \\leq 1 & \\beta = 0\n",
    "   \\end{cases}\n",
    "\\quad \\quad\n",
    "t_{l,j} =  \n",
    "\\begin{cases} \n",
    "      sign(\\beta_{l,j}) & \\beta_{l,j} \\neq 0 \\\\\n",
    "      t_{l,j} \\in [-1, 1] & \\beta_{l,j} = 0\n",
    "   \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice part about the group lasso part of the penalty is that it is only non-differentiable when the entirety of the group is zero. So, the subgradient method here will consist of a two part treatment,\n",
    "\n",
    "**Group Lasso Procedure**\n",
    "\n",
    "* When $\\beta_l > 0$, take the standard gradient of the objective\n",
    "\n",
    "* When $\\beta_l = 0$, take the gradient with respect to $\\beta_l$ of the square loss but allow the update to be given by,\n",
    "\n",
    "$$ \\frac{\\min \\{ \\| \\nabla_{\\beta_l} \\|_2 - \\lambda_1, 0 \\}}{\\| \\nabla_{\\beta_l} \\|_2} \\nabla_{\\beta_l}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the intuition here is that we are only interested in updating $\\beta_l$ if it will not eventualy arrive back at 0. This is a legal subgradient at $\\beta_l = 0$ and so can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sparse Group Lasso Procedure**\n",
    "\n",
    "* When $\\beta_{l,k} > 0$ for every peptide in the group, take the standard gradient of the objective\n",
    "\n",
    "* When individual $\\beta_{l,k} = 0$ take the gradient with respect to $\\beta_{l,k}$ of the square loss and the group penalty but allow the update to be given by,\n",
    "\n",
    "$$ \\frac{\\min \\{ | \\nabla_{\\beta_{l,k}} | - \\lambda_2, 0 \\}}{| \\nabla_{\\beta_{l,k}} |} \\nabla_{\\beta_{l,k}}$$\n",
    "\n",
    "* When $\\beta_l = 0$, take the gradient with respect to $\\beta_l$ of the square loss but allow the update to be given by a two step procedure,\n",
    "\n",
    "    1. $$  \\nabla_{\\beta_l}' = \n",
    "           \\frac{\\min \\{ \\| \\nabla_{\\beta_l} \\|_2 - \\lambda_1, 0 \\}}{\\| \\nabla_{\\beta_l} \\|_2} \\nabla_{\\beta_l}\n",
    "       $$\n",
    "    2. $$ \n",
    "          \\frac{\\min \\{ | \\nabla_{\\beta_{l},k}' | - \\lambda_2, 0 \\}}{| \\nabla_{\\beta_{l},k}'  |} \\nabla_{\\beta_{l},k}'\n",
    "       $$\n",
    "       \n",
    "The two step procedure gives the group penalty the ability to act first, but gives a second penalty on to the components of any group which passes the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradGL:\n",
    "    def __init__(self, l2_penalty, step_size=5e-10, max_iter=100):\n",
    "\n",
    "        self.l2_penalty = l2_penalty\n",
    "        self.step_size = step_size\n",
    "        self.max_iter = max_iter\n",
    "        self.X_ = None\n",
    "        self.y_ = None\n",
    "        self.group_map_ = None\n",
    "        self.coef_ = None\n",
    "        self.loss_ = None\n",
    "        \n",
    "    def preprocess_groups(self, groups):\n",
    "        idx = np.arange(len(groups))\n",
    "        group_map = {g: idx[groups == g] for g in np.unique(groups)}\n",
    "        return group_map\n",
    "    \n",
    "    def calculate_loss(self):\n",
    "        loss = np.sum(np.power(self.y_ - np.matmul(self.X_, self.coef_), 2))\n",
    "        for g in self.groups_:\n",
    "            group_select = self.groups_[g]\n",
    "            group_size = len(group_select)\n",
    "            group_norm = np.linalg.norm(self.coef_[group_select])\n",
    "            loss += self.l2_penalty * np.sqrt(group_size) * group_norm\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "    def fit(self, X, groups, y):\n",
    "        # Store groups\n",
    "        self.groups_ = self.preprocess_groups(groups)\n",
    "        \n",
    "        # Check X and y\n",
    "        self.X_, self.y_ = X, np.atleast_2d(y).T\n",
    "\n",
    "        # Set coefficients to zero\n",
    "        self.coef_ = 10 * np.abs(np.random.randn(self.X_.shape[1], 1)) #np.zeros([self.X_.shape[1], 1])\n",
    "        best_loss = np.inf\n",
    "        best_coef = None\n",
    "        tracker = 2\n",
    "        for i in range(self.max_iter):\n",
    "            cur_loss = self.calculate_loss()\n",
    "            if cur_loss < best_loss:\n",
    "                best_loss = cur_loss\n",
    "                best_coef = self.coef_.copy()\n",
    "                tracker = 2\n",
    "            else:\n",
    "                tracker -= 1\n",
    "                if tracker == 0:\n",
    "                    break\n",
    "            \n",
    "            loss_grad = -1. * np.matmul(X.T, (self.y_ - np.matmul(self.X_, self.coef_)))\n",
    "            \n",
    "            for g in self.groups_:\n",
    "                group_select = self.groups_[g]\n",
    "                group_size = len(group_select)\n",
    "                if np.any(self.coef_[group_select] > 0.):\n",
    "                    penalty_grad = self.coef_[self.groups_[g]] / np.linalg.norm(self.coef_[self.groups_[g]])\n",
    "                    loss_grad[group_select] += self.l2_penalty * np.sqrt(group_size) * penalty_grad\n",
    "                else:\n",
    "                    grad_norm = np.linalg.norm(loss_grad[group_select])\n",
    "                    grad_scale = max(grad_norm - self.l2_penalty * np.sqrt(group_size), 0.) / grad_norm\n",
    "                    loss_grad[group_select] = grad_scale * loss_grad[group_select]\n",
    "                    \n",
    "            self.coef_ -= self.step_size * loss_grad\n",
    "            self.coef_ = np.maximum(self.coef_, 0.)\n",
    "            \n",
    "        self.coef_ = best_coef.copy()\n",
    "        self.loss_ = best_loss\n",
    "        \n",
    "    def predict(self):\n",
    "        return np.matmul(self.X_, self.coef_) \n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 1 ms, total: 145 ms\n",
      "Wall time: 148 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = GradGL(l2_penalty=1e10, max_iter=1000, step_size=5e-10)\n",
    "model.fit(X, g, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.]\n",
      "1 [0.]\n",
      "2 [0.]\n",
      "3 [0.]\n",
      "3 [0.]\n",
      "4 [0.]\n",
      "5 [0.]\n",
      "6 [0.]\n",
      "6 [0.]\n",
      "7 [0.]\n",
      "8 [0.]\n",
      "9 [0.]\n",
      "9 [0.]\n",
      "10 [0.]\n",
      "11 [0.]\n",
      "12 [0.]\n",
      "13 [0.]\n",
      "14 [0.]\n",
      "15 [0.]\n",
      "16 [0.]\n",
      "16 [0.]\n",
      "17 [0.]\n",
      "18 [0.]\n",
      "18 [0.]\n",
      "19 [0.]\n",
      "20 [313.6376919]\n",
      "20 [297.53250139]\n",
      "21 [0.]\n",
      "22 [0.]\n",
      "22 [0.]\n",
      "23 [0.]\n",
      "24 [0.]\n",
      "25 [0.]\n",
      "26 [0.]\n",
      "26 [0.]\n",
      "27 [0.]\n",
      "28 [0.]\n",
      "29 [0.]\n",
      "29 [0.]\n",
      "30 [58.24439465]\n",
      "30 [0.]\n",
      "31 [0.]\n",
      "31 [0.]\n",
      "32 [0.]\n",
      "33 [0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(g)):\n",
    "    print(g[i], model.coef_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5IAAAJCCAYAAACyOPiJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3X+srHddJ/D3x3uF7coiIAXZtmy7a90s4qbiTWFDNES0FNZQdiObEiONsulqwHiyuxGQPzAoWdRVrmSVDSuNxaCFVQmNKVsqP3aziWBv4QgWxF4R5UoXiq0IQTHF7/5xngvnns65d75z5pxnnpnXKzk5M995Zp7PPL/meT/fZ56p1loAAABgXl8zdgEAAABMiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoMvxsQtYFY997GPb5ZdfPnYZAAAAo7jrrrs+21q7eJ5hBcnB5ZdfnlOnTo1dBgAAwCiq6s/mHdaprQAAAHQRJAEAAOgiSAIAANBFkAQAAKCLIAkAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAADoIkgCAADQRZAEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAADLa2trK1tTV2GbDyjo9dAAAArIrt7e2xS4BJ0CMJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQRJAEAAOgiSAIAANBFkAQAAKCLIAkAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAADoIkgCAADQRZAEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgDACLa2trK1tTV2GbCQ42MXAAAAm2h7e3vsEmBheiQBAADoIkgCAADQRZAEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAcB5bW1vZ2toauwxYKcfHLgAAAFbZ9vb22CXAytEjCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQRJAEAAOgiSAIAANBFkAQAAKCLIAkAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAADoIkgCAADQZWlBsqqOVdUHq+p3hvtXVNX7q+qeqnpLVT1saH/4cP/08Pjlu17j5UP7x6rqWbvarx3aTlfVy3a1d48DAACAg1lmj+SPJfnorvs/k+S1rbUrkzyQ5EVD+4uSPNBa+6Ykrx2GS1U9Kcn1Sb4lybVJfnkIp8eS/FKSZyd5UpIXDMN2jwMAAICDW0qQrKpLk/zrJL8y3K8k35XkN4dBbk7yvOH2dcP9DI8/cxj+uiS3tNa+1Fr70ySnk1w9/J1urX28tfZ3SW5Jct2C4wAAAOCAltUjeTLJjyf5++H+NyT5q9bag8P9M0kuGW5fkuSTSTI8/rlh+K+073nOfu2LjOMcVXVjVZ2qqlP33Xdf/7sGAADYQAcOklX1vUk+01q7a3fzjEHbBR5bVvuFxv/Vhtbe0Fo70Vo7cfHFF894CgAAAHsdX8JrPD3Jc6vqOUn+QZJHZqeH8lFVdXzoEbw0yaeG4c8kuSzJmao6nuTrk9y/q/2s3c+Z1f7ZBcYBAADAAR24R7K19vLW2qWttcuzc7Gcd7fWvj/Je5J83zDYDUnePty+dbif4fF3t9ba0H79cMXVK5JcmeT3k9yZ5MrhCq0PG8Zx6/Cc3nEAAABwQMvokdzPS5PcUlU/neSDSd44tL8xya9V1ens9BJenySttbur6q1JPpLkwSQvbq19OUmq6iVJbk9yLMlNrbW7FxkHAAAAB7fUINlae2+S9w63P56dK67uHeZvkzx/n+e/OsmrZ7TfluS2Ge3d4wAAAOBglvk7kgAAAGwAQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAADoIkgCAADQRZAEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQRJAEAAOgiSAIAANBFkAQAgBWwtbWVra2tscuAuRwfuwAAACDZ3t4euwSYmx5JAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQRJAEAAOgiSAIAANBFkAQAAKCLIAkAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAADoIkgCAADQRZAEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQRJAEAAOgiSAIAANBFkAQAAKCLIAkAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAFbc1tZWtra2xi4D4CuOj10AAADnt729PXYJAOfQIwkAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAADoIkgCkMTv1AEA8/M7kgAk8Tt1AMD89EgCAADQRZAEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQRJAEAAOgiSAIAANBFkAQAAKCLIAkAAEAXQRIAAIAugiQAAABdBEkAgA2wtbWVra2tsctYC6YlJMfHLgAAgMO3vb09dglrw7QEPZIAAAB0EiQBACbGqZXA2A4cJKvqsqp6T1V9tKrurqofG9ofU1V3VNU9w/9HD+1VVa+rqtNV9aGqesqu17phGP6eqrphV/u3V9WHh+e8rqpq0XHAVNlpAOCs7e1tp1cCo1pGj+SDSf5Ta+1fJHlakhdX1ZOSvCzJu1prVyZ513A/SZ6d5Mrh78Ykr092QmGSVyZ5apKrk7zybDAchrlx1/OuHdq7xgFTZqcBAIBVceAg2Vq7t7X2geH255N8NMklSa5LcvMw2M1Jnjfcvi7Jm9qO9yV5VFU9IcmzktzRWru/tfZAkjuSXDs89sjW2u+11lqSN+15rZ5xAAAAcEBL/Y5kVV2e5NuSvD/J41tr9yY7YTPJ44bBLknyyV1POzO0na/9zIz2LDAOAAAADmhpQbKqHpHkt5Jstdb++nyDzmhrC7Sft5x5nlNVN1bVqao6dd99913gJQEAAEiWFCSr6muzEyLf3Fr77aH502dPJx3+f2ZoP5Pksl1PvzTJpy7QfumM9kXGcY7W2htaaydaaycuvvji+d8wAADABlvGVVsryRuTfLS19gu7Hro1ydkrr96Q5O272l84XFn1aUk+N5yWenuSa6rq0cNFdq5Jcvvw2Oer6mnDuF6457V6xgEAAMABHV/Cazw9yQ8k+XBVnb2k5E8keU2St1bVi5L8eZLnD4/dluQ5SU4n+WKSH0yS1tr9VfVTSe4chntVa+3+4faPJPnVJBclecfwl95xAGyKsz8Vc/LkyZErAQDW0YGDZGvt/2b2dxKT5Jkzhm9JXrzPa92U5KYZ7aeSPHlG+1/2jgNgE/ipGADgMC31qq0AAACsP0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQRJAEAAOgiSAIAANBFkIQ1trW1la2trbHLAABgzRwfuwDg8Gxvb49dAgAr6uyBxpMnT45cCTBFgiQAwAZysBE4CKe2AgAA0EWQBAAAoIsgCTBxLqoEABw135EEmDjfcwJYTy6IxCoTJAEAYAU5UMgqc2orAAAAXQRJAAAAugiSALDGXIwJgMPgO5IAsMZ8xwqAw6BHEiZKLwMAAGPRIwkTpZcBAICx6JEEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQhA2ytbWVra2tscsAJsa2A4C9jo9dAHB0tre3xy4BmCDbDgD20iMJABtE7yIAy6BHEgA2iN5FAJZBjyQAAABdBEkAAAC6CJIAAAB0ESQBAADoIkgCAADQRZAEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBNgAW1tb2draGrsMAGBNHB+7AAAO3/b29tglAABrRI8kAAAAXQRJAAA4AF8fYBM5tRUA6HZ2p/nkyZMjV0JifozN1wfYRIIkANDNjvNqMT+Ao+bUVoBD4lQnAGBd6ZEEOCR6CACAdaVHEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQRJAEAAOgiSAIAANBFkAQAAKCLIAkd/MA8AAAkx8cuAKbED8wDsM7OHiw9efLkyJUAq06QBAAgiQOmwPyc2goAAEAXQRIAYAbfiwfYn1NbAVac7yzBOJzmCbA/QRJgxdmZBQBWjVNbAQAA6CJIAsCG811AAHo5tRUANpzTpwHopUcSmIyp95pMvX7g4GwHgHWhRxJG4Cqc89k7nabeazL1+hdleWddLGNZ3tTtALB+BEkYgR2J+ZhO68F8ZF1YlgG+yqmtACNymhsAMEV6JAFGtEgPh1NFAYCxCZIAE+P0OmAeDjoBh0mQBABYQw46AYfJdyThAHy/DQCATaRHEg7A0V4ADsopqMAUCZIArDw72qvPPFqcg5JsEtuK9SFIApO2bh9I6/Z+lsWO9uozj4B52FasD0GSI2UnmWVbtw+kdXs/AMB6EiQ5UnaSAQeUWBfzLMuWd2BdCZIAHCkHlFgX8yzLlndgXfn5DwAmx0/v0MPysjjTbnGrNO3GqmWVpgHLp0cSgMnRy0MPy8viTLvFrdK0G6uWRce795TwRU4Rd1r54RMkYQXY2MHBWY+AedhWrL69AXSRQLpKQX5dCZKsnE3cwNvYLc8mLj/ssB5xELYdm8O2ApZDkGTl2MBzEJYfDmLqYWLq9Y9plbYd6zgf1/E9jcW0XN40MC0PRpAEYGPt3YlYpTCxiKnXz451nI9jvad1DArruHz0WtY0MC0PRpBkVOu4gR/rPa3jtFwG04XdlhEcx1ymprQ8T6lWxndYPUyCwuKmtg5fqN6pvZ8pECQ5VBdaaddxAz+1K6ONZdaycRgb+alNFw7XMoLjspapRZb3o1qel7EujjmdmJ4xe5gsYzumHsIvVO+Y72ddlzFBcgPMs/Ae1mWWl7HSLlL/IsMcVbBhx6xlY6yd5GUtY1OySu9nlWqZZRk7pmNuHxexSjuQ89SyjM8wNtcqLe9jWuVt0lE5qm3q1KfTWYLkBphnw3ChyyzPs8Af1oZ4kfoXGeawgs3UNxbzBOypBbFFLiu+yM7sKlul3qJZtSzjdcecH4ssY6vcQ7ksh3XQ76im9zymtB1YxLIOui7jIPe6T+t5HdVn8jzzfmrbpL3m+TxaxvSd+nQ6a62DZFVdm+QXkxxL8iuttdeMXNJkHdaO3iKOqodyWVb9NJsLbSDnCdjzHHiY2kZzGTuzU7dIr9qylt1lTMspnQaarO8R692Wsa1Y9YtsrNt2YK95PhOWdfD5Qp81U5vWR9XbtayDo/M8ZxMOuG/CPs+i1jZIVtWxJL+U5HuSnElyZ1Xd2lr7yLiVzW/RnqCj+rLxKn8X8LA2okflKGs7jA/mVZ6281rl71oclaPaOZk6gWRxm/AeN5H5OpvpMtvUp8vU6z+Iaq2NXcOhqKp/leQnW2vPGu6/PElaa/9l1vAnTpxop06dOsIKL+ydT/7WfMOXv5xHPOIRX2n7whe+kCRfadt7f1bbPfc/Okly5WMemPs5i4xnkVr23p9nmGXVMs9zLlTLrPqX8bpH9ZxFp8sqLS/LqGVq02VKz0mWs04va91bpelyVLUsst2dZx1ZpW31hYaZZ3lZ5D0u8rqLjGfMWqa2vB/WZ/thLe+HsZ+x6tvqZey/HNXyPqttWcvuXx47lmv+8MNZNVV1V2vtxDzDrm2PZJJLknxy1/0zSZ66e4CqujHJjUnyxCc+8egqm9NFF12UY3/zN+e0HTt27Lz3Z7V96e+/brj1wNzPWWQ8i9Sy9/48wyyrlnmec6FaZtW/jNc9qufMU++qLy/LqGVq02VKz0mWs04va91bpelyVLUsst2dZx1ZpW31hYaZZ3lZ5D0u8rqLjGfMWqa2vB/WZ/thLe+HsZ+x6tvqZey/HNXyPqttWcvuRRdd9JDnTc0690g+P8mzWmv/frj/A0mubq396KzhV7FHclme8Yyd/+9975hV7Nhby6za5hnmqFyolkVrW+R1juo5q+So6p/adJmaZczHqS/LY1pku3uh15j3dVfFItvLWW2HtR1exjCHVcs6WmRaHtbyflj7GYdRy7JeYxnLquX98PT0SH7NYRczojNJLtt1/9IknxqpFgAAgLWxzkHyziRXVtUVVfWwJNcnuXXkmgAAACZvbb8j2Vp7sKpekuT27Pz8x02ttbtHLgsAAGDy1jZIJklr7bYkt41dB6yKq64auwKAo2fbB7B8ax0kYdPt3Xla0d/6BThUtn0AyydIwkTsDYXzHGGf+s7TIu8ZADg4n7lciCAJE7E3FE49JM7jqN6zD0sAONcm7GdwMIIksPF8WALAwTkwu1kESUa16hucVa8PNpV1E2D1HNWBWZ8Bq0GQZFSr3hNkgwiradW3HZvIdgw4Kj4DVoMgCSvABhGYOtsxpsJBD1gOQRIAYIUIOodr70EP0xsWI0gCcKTstMH56d09WqY3LEaQhAOwQwz97LQBwPQJkhtgamFnSvXaIQYAYBMJkhtgamFnavUCh29KB5gApsh2ll6CJEfKRgpYhANMAIfLdpZegiRHapGNlPAJwCry+QRsMkGSlecIGUybnW3W1TyfT1Nb/qdWLzAeQRKAQ+VgEJtsasv/1OoFxiNIAsCG0wtlGgD0EiQBYMPphTINAHp9zdgFAAAAMC16JGEETqECAJiP/abVJEjCCJxCBQAwH/tNq0mQBAAAlm7RnkQ9kNMgSAIATJwdb1bRrJ7EeZZVPZDTIEhCBx/UAKwiO95MhWV1fQiS0MHGDwAA/PwHAAAAnQRJAAAAugiSAAAAdPEdSQCgm4uPAWw2QRIA6ObiYwCbzamtAAAAdNEjCQAwIqcJA1MkSAIAjMhpwsAUObUVAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6OKqrQAAc/AzHQBfJUgCAMzBz3QAfJVTWwEAAOgiSAIAANBFkAQAAKCLIAkAAEAXF9sBgA2yyJVHXa0UxmHdY5UJkgCwQRa58qirlcI4rHusMkESAOCI6GEC1oUgCQBwRPQwAevCxXYAAADoIkgCAADQRZAEAACgi+9IAgBsIBf+AQ5CkAQA2EAu/AMchFNbAQAA6KJHEgDWmNMXF2faAexPkASANeb0xcWt8rQTcoGxCZIAABOzyiEX2Ay+IwkAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAADo4uc/AAA2gN+eBJZJkAQA2AB+exJYJqe2AgAA0EWPJADAinNaKrBqBEkAgBXntFRg1QiSAADQQQ8xCJIAANBFDzG42A4AAACd9EgCAABrzenIyydIAgAAa83pyMvn1FYAAAC6CJIAAAB0ESQBAADoIkgCAADQRZAEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQRJAEAAOgiSAIAANBFkAQAAKCLIAkAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAADoIkgCAADQRZAEAACgy4GCZFX9XFX9UVV9qKreVlWP2vXYy6vqdFV9rKqetav92qHtdFW9bFf7FVX1/qq6p6reUlUPG9ofPtw/PTx++aLjAAAA4OAO2iN5R5Int9b+ZZI/TvLyJKmqJyW5Psm3JLk2yS9X1bGqOpbkl5I8O8mTkrxgGDZJfibJa1trVyZ5IMmLhvYXJXmgtfZNSV47DLfoOAAAADigAwXJ1to7W2sPDnffl+TS4fZ1SW5prX2ptfanSU4nuXr4O91a+3hr7e+S3JLkuqqqJN+V5DeH59+c5Hm7Xuvm4fZvJnnmMHzXOA7yPgEAAPiqZX5H8oeSvGO4fUmST+567MzQtl/7NyT5q12h9Gz7Oa81PP65YfjecQAAALAExy80QFX9bpJvnPHQK1prbx+GeUWSB5O8+ezTZgzfMju4tvMMf77X6h3HQ1TVjUluTJInPvGJswYBAABgjwsGydbad5/v8aq6Icn3Jnlma+1sYDuT5LJdg12a5FPD7Vntn03yqKo6PvQ67h7+7GudqarjSb4+yf0LjGPWe3tDkjckyYkTJ2aGTQAAAM510Ku2XpvkpUme21r74q6Hbk1y/XDF1SuSXJnk95PcmeTK4QqtD8vOxXJuHQLoe5J83/D8G5K8fddr3TDc/r4k7x6G7xrHQd4nAAAAX3XBHskL+G9JHp7kjp3r3+R9rbUfbq3dXVVvTfKR7Jzy+uLW2peTpKpekuT2JMeS3NRau3t4rZcmuaWqfjrJB5O8cWh/Y5Jfq6rT2emJvD5JFhwHAAAAB3SgIDn8JMd+j706yatntN+W5LYZ7R/PzhVX97b/bZLnL2McAAAAHNwyr9oKAADABhAkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQ5PnYBAABActVVY1cA8xMkAQBgBZw8OXYFMD+ntgIAANBFkAQAAKCLIAkAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAADoIkgCAADQRZAEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC7Hxy4AAADgqF111dgVTJsgCQAAbJyTJ8euYNqc2goAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0cdVWAAA4Dz8TAQ8lSAIAwHn4mQh4KKe2AgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQRJAEAAOgiSAIAANDl+NgFAADAJrrqqrErgMUJkgAAMIKTJ8euYH0J6YdPkAQAANaKkH74fEcSAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALoIkgAAAHQRJAEAAOgiSAIAANBFkAQAAKCLIAkAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAADoIkgCAADQRZAEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAugiSAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF2Oj10AAACsiquuGrsCmAZBEgAABidPjl0BTINTWwEAAOgiSAIAANBFkAQAAKCLIAkAAEAXQRIAAIAugiQAAABdBEkAAAC6CJIAAAB0ESQBAADoIkgCAADQRZAEAACgiyAJAABAF0ESAACALoIkAAAAXQRJAAAAuiwlSFbVf66qVlWPHe5XVb2uqk5X1Yeq6im7hr2hqu4Z/m7Y1f7tVfXh4Tmvq6oa2h9TVXcMw99RVY9edBwAAAAc3IGDZFVdluR7kvz5ruZnJ7ly+LsxyeuHYR+T5JVJnprk6iSvPBsMh2Fu3PW8a4f2lyV5V2vtyiTvGu4vOg4AAAAOaBk9kq9N8uNJ2q6265K8qe14X5JHVdUTkjwryR2ttftbaw8kuSPJtcNjj2yt/V5rrSV5U5Ln7Xqtm4fbN+9pn3scS3ifAAAA5IBBsqqem+QvWmt/sOehS5J8ctf9M0Pb+drPzGhPkse31u5NkuH/4xYcx6z6b6yqU1V16r777jvPOwUAAOCs4xcaoKp+N8k3znjoFUl+Isk1s542o60t0H7e0g76Wq21NyR5Q5KcOHHiQuMDAAAgcwTJ1tp3z2qvqm9NckWSPxiui3Npkg9U1dXZ6QW8bNfglyb51ND+jD3t7x3aL50xfJJ8uqqe0Fq7dzh19TNDe+84AAAAWIKFT21trX24tfa41trlrbXLsxPgntJa+39Jbk3ywuHKqk9L8rnhtNTbk1xTVY8eLoBzTZLbh8c+X1VPG67W+sIkbx9GdWuSs1devWFP+9zjWPR9AgAAcK4L9kgu6LYkz0lyOskXk/xgkrTW7q+qn0py5zDcq1pr9w+3fyTJrya5KMk7hr8keU2St1bVi7JzZdjnH2AcAAAAHFDtXCSVqrovyZ+NXccMj03y2bGLoJv5Nj3m2fSYZ9Nkvk2PeTY95tk0rcJ8+yettYvnGVCQXHFVdaq1dmLsOuhjvk2PeTY95tk0mW/TY55Nj3k2TVObb8v4HUkAAAA2iCAJAABAF0Fy9b1h7AJYiPk2PebZ9Jhn02S+TY95Nj3m2TRNar75jiQAAABd9EgCAADQRZBcYVV1bVV9rKpOV9XLxq6Hh6qqy6rqPVX10aq6u6p+bGj/yar6i6raHv6eM3atnKuqPlFVHx7mz6mh7TFVdUdV3TP8f/TYdbKjqv75rvVpu6r+uqq2rGurpapuqqrPVNUf7mqbuV7VjtcNn3EfqqqnjFf5Zttnvv1cVf3RMG/eVlWPGtovr6q/2bXO/ffxKt9c+8yzfbeHVfXyYV37WFU9a5yqN9s+8+wtu+bXJ6pqe2ifxHrm1NYVVVXHkvxxku9JcibJnUle0Fr7yKiFcY6qekKSJ7TWPlBV/yjJXUmel+TfJflCa+2/jlog+6qa+l99AAAERElEQVSqTyQ50Vr77K62n01yf2vtNcPBm0e31l46Vo3MNmwf/yLJU5P8YKxrK6OqvjPJF5K8qbX25KFt5no17OT+aJLnZGde/mJr7alj1b7J9plv1yR5d2vtwar6mSQZ5tvlSX7n7HCMY5959pOZsT2sqicl+Y0kVyf5x0l+N8k3t9a+fKRFb7hZ82zP4z+f5HOttVdNZT3TI7m6rk5yurX28dba3yW5Jcl1I9fEHq21e1trHxhufz7JR5NcMm5VHMB1SW4ebt+cnYMCrJ5nJvmT1tqfjV0I52qt/Z8k9+9p3m+9ui47O1Sttfa+JI8aDs5xxGbNt9baO1trDw5335fk0iMvjH3ts67t57okt7TWvtRa+9Mkp7Ozn8kROt88q6rKTifEbxxpUQckSK6uS5J8ctf9MxFQVtpw9Ojbkrx/aHrJcErQTU6RXEktyTur6q6qunFoe3xr7d5k5yBBkseNVh3nc33O/bC1rq22/dYrn3PT8UNJ3rHr/hVV9cGq+t9V9R1jFcVMs7aH1rXV9x1JPt1au2dX28qvZ4Lk6qoZbc5DXlFV9Ygkv5Vkq7X210len+SfJbkqyb1Jfn7E8pjt6a21pyR5dpIXD6ecsOKq6mFJnpvkfw5N1rXp8jk3AVX1iiQPJnnz0HRvkie21r4tyX9M8utV9cix6uMc+20PrWur7wU59wDpJNYzQXJ1nUly2a77lyb51Ei1cB5V9bXZCZFvbq39dpK01j7dWvtya+3vk/yPOIVk5bTWPjX8/0ySt2VnHn367Kl1w//PjFch+3h2kg+01j6dWNcmYr/1yufciquqG5J8b5Lvb8NFNYbTI/9yuH1Xkj9J8s3jVclZ59keWtdWWFUdT/Jvk7zlbNtU1jNBcnXdmeTKqrpiOAJ/fZJbR66JPYZz2t+Y5KOttV/Y1b77ez7/Jskf7n0u46mqrxsujpSq+rok12RnHt2a5IZhsBuSvH2cCjmPc47aWtcmYb/16tYkLxyu3vq07Fxk4t4xCuShquraJC9N8tzW2hd3tV88XPAqVfVPk1yZ5OPjVMlu59ke3prk+qp6eFVdkZ159vtHXR/7+u4kf9RaO3O2YSrr2fGxC2C24SppL0lye5JjSW5qrd09clk81NOT/ECSD5+9ZHOSn0jygqq6KjunjnwiyX8Ypzz28fgkb9s5DpDjSX69tfa/qurOJG+tqhcl+fMkzx+xRvaoqn+YnStZ716ffta6tjqq6jeSPCPJY6vqTJJXJnlNZq9Xt2Xniq2nk3wxO1fgZQT7zLeXJ3l4kjuGbeX7Wms/nOQ7k7yqqh5M8uUkP9xam/eiLyzJPvPsGbO2h621u6vqrUk+kp3TlF/siq1Hb9Y8a629MQ/93n8ykfXMz38AAADQxamtAAAAdBEkAQAA6CJIAgAA0EWQBAAAoIsgCQAAQBdBEgAAgC6CJAAAAF0ESQAAALr8fzHItiNd/4+JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "true_y = model.y_.flatten()\n",
    "plt.stem(np.arange(len(true_y)), true_y, \n",
    "        markerfmt=\" \",  linefmt=\"black\")\n",
    "\n",
    "pred_y = -model.predict()[:, 0]\n",
    "print(pred_y.shape)\n",
    "plt.stem(np.arange(len(pred_y)), pred_y, \n",
    "        markerfmt=\" \",  linefmt=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradSGL:\n",
    "    def __init__(self, l2_penalty, l1_penalty, step_size=5e-10, max_iter=100):\n",
    "\n",
    "        self.l2_penalty = l2_penalty\n",
    "        self.l1_penalty = l1_penalty\n",
    "        self.step_size = step_size\n",
    "        self.max_iter = max_iter\n",
    "        self.X_ = None\n",
    "        self.y_ = None\n",
    "        self.group_map_ = None\n",
    "        self.coef_ = None\n",
    "        self.loss_ = None\n",
    "        \n",
    "    def preprocess_groups(self, groups):\n",
    "        idx = np.arange(len(groups))\n",
    "        group_map = {g: idx[groups == g] for g in np.unique(groups)}\n",
    "        return group_map\n",
    "    \n",
    "    def calculate_loss(self):\n",
    "        loss = np.sum(np.power(self.y_ - np.matmul(self.X_, self.coef_), 2))\n",
    "        for g in self.groups_:\n",
    "            group_select = self.groups_[g]\n",
    "            group_size = len(group_select)\n",
    "            group_norm = np.linalg.norm(self.coef_[group_select])\n",
    "            loss += self.l2_penalty * np.sqrt(group_size) * group_norm\n",
    "        loss += self.l1_penalty * np.sum(self.coef_)\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "    def fit(self, X, groups, y):\n",
    "        # Store groups\n",
    "        self.groups_ = self.preprocess_groups(groups)\n",
    "        \n",
    "        # Check X and y\n",
    "        self.X_, self.y_ = X, np.atleast_2d(y).T\n",
    "\n",
    "        # Set coefficients to zero\n",
    "        self.coef_ = 10 * np.abs(np.random.randn(self.X_.shape[1], 1)) #np.zeros([self.X_.shape[1], 1])\n",
    "        best_loss = np.inf\n",
    "        best_coef = None\n",
    "        tracker = 2\n",
    "        for i in range(self.max_iter):\n",
    "            cur_loss = self.calculate_loss()\n",
    "            if cur_loss < best_loss:\n",
    "                best_loss = cur_loss\n",
    "                best_coef = self.coef_.copy()\n",
    "                tracker = 2\n",
    "            else:\n",
    "                tracker -= 1\n",
    "                if tracker == 0:\n",
    "                    break\n",
    "            \n",
    "            loss_grad = -1. * np.matmul(X.T, (self.y_ - np.matmul(self.X_, self.coef_)))\n",
    "            \n",
    "            for g in self.groups_:\n",
    "                group_select = self.groups_[g]\n",
    "                group_size = len(group_select)\n",
    "                if np.any(self.coef_[group_select] > 0.):\n",
    "                    penalty_grad = self.coef_[self.groups_[g]] / np.linalg.norm(self.coef_[self.groups_[g]])\n",
    "                    loss_grad[group_select] += self.l2_penalty * np.sqrt(group_size) * penalty_grad\n",
    "                else:\n",
    "                    grad_norm = np.linalg.norm(loss_grad[group_select])\n",
    "                    grad_scale = max(grad_norm - self.l2_penalty * np.sqrt(group_size), 0.) / grad_norm\n",
    "                    loss_grad[group_select] = grad_scale * loss_grad[group_select]\n",
    "            \n",
    "            # l1 penalty\n",
    "            coef_zero = self.coef_ == 0\n",
    "            grad_zero = loss_grad == 0\n",
    "            defined = np.array(np.logical_and(~coef_zero, ~grad_zero)).flatten()\n",
    "            if np.any(defined):\n",
    "                signs = np.sign(self.coef_[defined])\n",
    "                loss_grad[defined] += signs * self.l1_penalty\n",
    "                \n",
    "            undefined = np.array(np.logical_and(coef_zero, ~grad_zero)).flatten()\n",
    "            if np.any(undefined):\n",
    "                zero_pos = np.array(loss_grad[undefined])\n",
    "                scale = np.maximum(np.abs(zero_pos) - self.l1_penalty, 0.) / np.abs(zero_pos)\n",
    "                loss_grad[undefined] = scale * zero_pos\n",
    "                    \n",
    "            self.coef_ -= self.step_size * loss_grad\n",
    "            self.coef_ = np.maximum(self.coef_, 0.)\n",
    "            \n",
    "        self.coef_ = best_coef.copy()\n",
    "        self.loss_ = best_loss\n",
    "        \n",
    "    def predict(self):\n",
    "        return np.matmul(self.X_, self.coef_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 901 ms, sys: 2 ms, total: 903 ms\n",
      "Wall time: 909 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = GradSGL(l2_penalty=1e10, l1_penalty=1e10, max_iter=1000, step_size=5e-10)\n",
    "model.fit(X, g, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.]\n",
      "1 [0.]\n",
      "2 [0.]\n",
      "3 [0.]\n",
      "3 [0.]\n",
      "4 [0.]\n",
      "5 [0.]\n",
      "6 [0.]\n",
      "6 [0.]\n",
      "7 [0.]\n",
      "8 [0.]\n",
      "9 [0.]\n",
      "9 [0.]\n",
      "10 [0.]\n",
      "11 [0.]\n",
      "12 [0.]\n",
      "13 [0.]\n",
      "14 [0.]\n",
      "15 [0.]\n",
      "16 [0.]\n",
      "16 [0.]\n",
      "17 [0.]\n",
      "18 [0.]\n",
      "18 [0.]\n",
      "19 [0.]\n",
      "20 [301.63990363]\n",
      "20 [269.22419677]\n",
      "21 [0.]\n",
      "22 [0.]\n",
      "22 [0.]\n",
      "23 [0.]\n",
      "24 [0.]\n",
      "25 [0.]\n",
      "26 [0.]\n",
      "26 [0.]\n",
      "27 [0.]\n",
      "28 [0.]\n",
      "29 [0.]\n",
      "29 [0.]\n",
      "30 [0.]\n",
      "30 [0.]\n",
      "31 [0.]\n",
      "31 [0.]\n",
      "32 [0.]\n",
      "33 [0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(g)):\n",
    "    print(g[i], model.coef_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
